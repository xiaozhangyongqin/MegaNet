import torch
import torch.nn.functional as F
import torch.nn as nn
import math
import numpy as np
from SelfAttention import ScaledDotProductAttention



class AGCN(nn.Module):
    def __init__(self, dim_in, dim_out, cheb_k):
        super(AGCN, self).__init__()
        self.cheb_k = cheb_k
        self.weights = nn.Parameter(torch.FloatTensor(2 * cheb_k * dim_in, dim_out))  # 2 is the length of support
        self.bias = nn.Parameter(torch.FloatTensor(dim_out))
        nn.init.xavier_normal_(self.weights)
        nn.init.constant_(self.bias, val=0)

    def forward(self, x, supports):
        x_g = []
        support_set = []
        for support in supports:
            support_ks = [torch.eye(support.shape[0]).to(support.device), support]
            for k in range(2, self.cheb_k):
                support_ks.append(torch.matmul(2 * support, support_ks[-1]) - support_ks[-2])
            support_set.extend(support_ks)
        for support in support_set:
            x_g.append(torch.einsum("nm,bmc->bnc", support, x))
        x_g = torch.cat(x_g, dim=-1)  # B, N, 2 * cheb_k * dim_in
        x_gconv = torch.einsum('bni,io->bno', x_g, self.weights) + self.bias  # b, N, dim_out
        return x_gconv, support_set[-1]


class AGCRNCell(nn.Module):
    def __init__(self, node_num, dim_in, dim_out, cheb_k):
        super(AGCRNCell, self).__init__()
        self.node_num = node_num
        self.hidden_dim = dim_out
        self.gate = AGCN(dim_in + self.hidden_dim, 2 * dim_out, cheb_k)
        self.update = AGCN(dim_in + self.hidden_dim, dim_out, cheb_k)

    def forward(self, x, state, supports):
        # x: B, num_nodes, input_dim
        # state: B, num_nodes, hidden_dim
        state = state.to(x.device)
        input_and_state = torch.cat((x, state), dim=-1)
        z_r, support_last = self.gate(input_and_state, supports)
        z_r = torch.sigmoid(z_r)
        z, r = torch.split(z_r, self.hidden_dim, dim=-1)
        candidate = torch.cat((x, z * state), dim=-1)
        hc, _ = self.update(candidate, supports)
        hc = torch.tanh(hc)
        h = r * state + (1 - r) * hc
        return h, support_last

    def init_hidden_state(self, batch_size):
        return torch.zeros(batch_size, self.node_num, self.hidden_dim)

class ADCRNN_Decoder(nn.Module):
    def __init__(self, node_num, dim_in, dim_out, cheb_k, num_layers):
        super(ADCRNN_Decoder, self).__init__()
        assert num_layers >= 1, 'At least one DCRNN layer in the Decoder.'
        self.node_num = node_num
        self.input_dim = dim_in
        self.num_layers = num_layers
        self.dcrnn_cells = nn.ModuleList()
        self.dcrnn_cells.append(AGCRNCell(node_num, dim_in, dim_out, cheb_k))
        for _ in range(1, num_layers):
            self.dcrnn_cells.append(AGCRNCell(node_num, dim_out, dim_out, cheb_k))

    def forward(self, xt, init_state, supports):
        # xt: (B, N, D)
        # init_state: (num_layers, B, N, hidden_dim)
        assert xt.shape[1] == self.node_num and xt.shape[2] == self.input_dim
        current_inputs = xt
        output_hidden = []
        for i in range(self.num_layers):
            state, _ = self.dcrnn_cells[i](current_inputs, init_state[i], supports)
            output_hidden.append(state)
            current_inputs = state
        return current_inputs, output_hidden


class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, embed_dim).float()
        pe.require_grad = False

        position = torch.arange(0, max_len).float().unsqueeze(1)
        div_term = (torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim)).exp()

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        pe = self.pe[:, :x.size(1)].unsqueeze(2).repeat(x.size(0), 1, x.size(2), 1)
        return pe.detach()
        # return self.pe[:, :x.size(1)].unsqueeze(2).expand_as(x).detach()


class MSTGRN(nn.Module):
    def __init__(self, num_nodes, input_dim, output_dim, horizon, rnn_units, num_layers=1, cheb_k=3,
                 ycov_dim=2, mem_num=20, mem_dim=72, cl_decay_steps=2000, use_curriculum_learning=True):
        super(MegaCRN, self).__init__()
        self.num_nodes = num_nodes
        self.input_dim = input_dim
        self.rnn_units = rnn_units
        self.output_dim = output_dim
        self.horizon = horizon
        self.num_layers = num_layers
        self.cheb_k = cheb_k
        self.ycov_dim = ycov_dim
        self.cl_decay_steps = cl_decay_steps
        self.use_curriculum_learning = use_curriculum_learning
        self.minute_size = 1440
        self.weekday_size = 7
        self.embed_dim = 48
        self.adaptive_embedding_dim = 16
        self.att_dim = self.embed_dim + self.adaptive_embedding_dim  # 64
        self.batch = 64
        # position_encoding
        self.position_encoding = PositionalEncoding(self.embed_dim)
        self.daytime_embedding = nn.Embedding(self.minute_size, self.embed_dim)
        self.weekday_embedding = nn.Embedding(self.weekday_size, self.embed_dim)
        # memory
        self.mem_num = mem_num
        self.mem_dim = mem_dim
        # self.memory = self.construct_memory()
        self.memory1 = self.construct_memory1()

        # selfattention
        self.att = ScaledDotProductAttention(d_model=self.att_dim, d_k=self.att_dim, d_v=self.att_dim, h=4)
        self.att_time = ScaledDotProductAttention(d_model=self.att_dim, d_k=self.att_dim, d_v=self.att_dim, h=4)

        self.sp_linear = nn.Linear(self.num_nodes, self.att_dim)
        self.linear1 = nn.Linear(self.att_dim * 2, self.att_dim)
        # 寻找图特征
        self.adaptive_embedding = nn.init.xavier_uniform_(
            nn.Parameter(torch.empty(12, self.num_nodes, self.adaptive_embedding_dim))
        )

        # 对x进行处理
        self.value_liner = nn.Linear(1, self.embed_dim)

        self.decoder_dim = self.rnn_units*2

        self.decoder = ADCRNN_Decoder(self.num_nodes, self.output_dim + self.ycov_dim + self.att_dim*2, self.decoder_dim,
                                      self.cheb_k, self.num_layers)
        self.reg = nn.Conv2d(
            in_channels=12, out_channels=1, kernel_size=(1, 1), bias=True)
        self.reg1 = nn.Conv2d(
            in_channels=12, out_channels=1, kernel_size=(1, 1), bias=True)
        # # output
        self.proj = nn.Sequential(nn.Linear(self.decoder_dim, self.output_dim, bias=True))

    def compute_sampling_threshold(self, batches_seen):
        return self.cl_decay_steps / (self.cl_decay_steps + np.exp(batches_seen / self.cl_decay_steps))

    # def construct_memory(self):
    #     memory_dict = nn.ParameterDict()
    #     memory_dict['Memory'] = nn.Parameter(torch.randn(self.mem_num, self.mem_dim), requires_grad=True)  # (M, d)
    #     memory_dict['Wq'] = nn.Parameter(torch.randn(self.rnn_units, self.mem_dim),
    #                                      requires_grad=True)  # project to query
    #     memory_dict['We1'] = nn.Parameter(torch.randn(self.num_nodes, self.mem_num),
    #                                       requires_grad=True)  # project memory to embedding
    #     memory_dict['We2'] = nn.Parameter(torch.randn(self.num_nodes, self.mem_num),
    #                                       requires_grad=True)  # project memory to embedding
    #     for param in memory_dict.values():
    #         nn.init.xavier_normal_(param)
    #     return memory_dict

    # def query_memory(self, h_t: torch.Tensor):
    #     query = h_t
    #     # self.memory['Memory'] = nn.Parameter(self.att_memory(self.memory['Memory'].unsqueeze(0)).squeeze(0))
    #     att_score = torch.softmax(torch.matmul(query, self.memory['Memory'].t()), dim=-1)  # alpha: (B, N, M)
    #     value = torch.matmul(att_score, self.memory['Memory'])  # (B, N, d)
    #     # 获取top 2 相关性最高的
    #     _, ind = torch.topk(att_score, k=3, dim=-1)
    #     pos = self.memory['Memory'][ind[:, :, 0]]  # B, N, d
    #     neg1 = self.memory['Memory'][ind[:, :, 1]]  # B, N, d
    #     neg2 = self.memory['Memory'][ind[:, :, 1]]  # B, N, d
    #     return value, query, pos, neg1, neg2

    def construct_memory1(self):
        memory_dict = nn.ParameterDict()
        memory_dict['Memory'] = nn.Parameter(torch.randn(self.mem_num, self.mem_dim*2), requires_grad=True)  # (M, d)
        memory_dict['We1'] = nn.Parameter(torch.randn(self.num_nodes, self.mem_num),
                                          requires_grad=True)  # project memory to embedding
        memory_dict['We2'] = nn.Parameter(torch.randn(self.num_nodes, self.mem_num),
                                          requires_grad=True)  # project memory to embedding
        for param in memory_dict.values():
            nn.init.xavier_normal_(param)
        return memory_dict

    def query_memory1(self, h_t: torch.Tensor):
        # query = torch.matmul(h_t, self.memory['Wq'])     # (B, N, d)
        query = h_t
        # self.memory['Memory'] = nn.Parameter(self.att_memory(self.memory['Memory'].unsqueeze(0)).squeeze(0))
        att_score = torch.softmax(torch.matmul(query, self.memory1['Memory'].t()), dim=-1)  # alpha: (B, N, M)
        value = torch.matmul(att_score, self.memory1['Memory'])  # (B, N, d)
        # 获取top 2 相关性最高的
        _, ind = torch.topk(att_score, k=3, dim=-1)
        pos = self.memory1['Memory'][ind[:, :, 0]]  # B, N, d
        neg1 = self.memory1['Memory'][ind[:, :, 1]]  # B, N, d
        neg2 = self.memory1['Memory'][ind[:, :, 1]]  # B, N, d
        return value, query, pos, neg1, neg2

    def forward(self, x, x_cov, y_cov, labels=None, batches_seen=None):
        # x: B T N D : (64, 12, 170 , 1) PEMSD8
        # y_cov: B T N D : (64, 12, 170 , 8) PEMSD8
        # x = x.to('cuda')
        origin_x = x
        origin_x_cov = x_cov

        node_embeddings1 = torch.matmul(self.memory1['We1'], self.memory1['Memory'])
        node_embeddings2 = torch.matmul(self.memory1['We2'], self.memory1['Memory'])
        g1 = F.softmax(F.relu(torch.mm(node_embeddings1, node_embeddings2.T)), dim=-1)
        g2 = F.softmax(F.relu(torch.mm(node_embeddings2, node_embeddings1.T)), dim=-1)
        supports = [g1, g2]
        # init_state = self.encoder.init_hidden(x.shape[0])

        x = self.value_liner(origin_x)  # B T N 24
        x = x + self.position_encoding(origin_x)
        # print(origin_y_cov.shape)
        # print(self.daytime_embedding((origin_y_cov * self.minute_size)).shape)
        x_day = self.daytime_embedding((origin_x_cov[..., 0] * self.minute_size).round().long())  # 24
        x = x + x_day
        x_week = self.weekday_embedding((origin_x_cov[..., 1]).long())  # 24
        x_adpative = self.adaptive_embedding.expand(
            size=(x.shape[0], *self.adaptive_embedding.shape)
        )  # dim = 64
        x = x + x_week
        
        x = torch.cat([x, x_adpative], dim=-1)  # 
        x_att = self.att(x, x, x)
        x_att_time = self.att_time(x.permute(0, 2, 1, 3), x.permute(0, 2, 1, 3), x.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)

        mut_x = x_att * x_att_time
        sum_x = x_att + x_att_time

        a = torch.cat([sum_x, mut_x], dim=-1)

        h_att_list, query_list, pos_list, neg1_list, neg2_list = [], [], [], [], []
        for t in range(self.horizon):
            h_att1, query1, pos1, neg11, neg21 = self.query_memory1(a[:, t, ...])
            h_att_list.append(h_att1)
            query_list.append(query1)
            pos_list.append(pos1)
            neg1_list.append(neg11)
            neg2_list.append(neg21)
        h_att1 = torch.stack(h_att_list, dim=1)
        h_att11 = h_att1.detach().cpu().numpy()
        np.save('h_att11.npy', h_att11)
        query1 = self.reg(torch.stack(query_list, dim=1))
        pos1 = self.reg(torch.stack(pos_list, dim=1))
        neg11 = self.reg(torch.stack(neg1_list, dim=1))
        neg21 = self.reg(torch.stack(neg2_list, dim=1))

        h_t = self.reg1(h_att1)[:, 0, ...]
        ht_list = [h_t] * self.num_layers
        go = torch.zeros((x.shape[0], self.num_nodes, self.output_dim), device=x.device)
        out = []

        for t in range(self.horizon):
            h_de, ht_list = self.decoder(torch.cat([go, y_cov[:, t, ...], h_att1[:, t, ...]], dim=-1),
                                         ht_list, supports)  # 64  + 64

            go = self.proj(h_de)
            out.append(go),
            # 是否用真实值
            if self.training and self.use_curriculum_learning:
                c = np.random.uniform(0, 1)
                if c < self.compute_sampling_threshold(batches_seen):
                    go = labels[:, t, ...]
        output = torch.stack(out, dim=1)

        return output, h_att1, query1, pos1, neg11, neg21
